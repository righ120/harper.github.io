---
title : thu_20180913_3
layout : post
tags : [Word_Sense_Embedding]
---

<h3>Deep contextualized word representations (NAACL 2018) </h3>


<p><b>Matthew E. Peters<sup>†</sup> , Mark Neumann<sup>†</sup> , Mohit Iyyer<sup>†</sup> , Matt Gardner<sup>†</sup>,</b><br/>
<em>{matthewp,markn,mohiti,mattg}@allenai.org </em>

<b>Christopher Clark<sup>*</sup> , Kenton Lee<sup>*</sup> , Luke Zettlemoyer<sup>†</sup></b>
<em>{csquared,kentonl,lsz}@cs.washington.edu </em>
<sup>†</sup>Allen Institute for Artificial Intelligence <br/>
<sup>*</sup>Paul G. Allen School of Computer Science & Engineering, University of Washington
 </p>
<hr />
<p>
Bi-directional language model 의 internal state를 word vector로 사용 --> syntax, semantics, context를 반영며 동시에 6개의 NLP task 에서 높은 성능을 보이는 것을 실험적으로 증명함
</p>