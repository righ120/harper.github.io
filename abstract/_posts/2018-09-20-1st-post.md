---
title : thu_20180920_1
layout : post
tags : [MT_Training_Trick]
---

<h3> Layer Normalization (arXive 2016) </h3>


<p><b>Jimmy Lei Ba</b> <br/>
University of Toronto <br/>
<em>jimmy@psi.toronto.edu</em> <br/>

<b>Jamie Ryan Kiros </b> <br/>
University of Toronto <br/>
<em>rkiros@cs.toronto.edu</em> <br/>

<b>Geoffrey E. Hinton</b>
University of Toronto<br/>
and Google Inc.<br/>
<em>hinton@cs.toronto.edu</em><br/>

</p>

<hr />
<p>
기존 batch normalization은 mini batch size 에 의존적이며 RNN에 어떻게 적용할지 명확하지 않다. 이 논문에서는 batch normalization 을 변형하여 한 개의 training case 에서 한 layer 의 모든 input 을 sum 하여 normalization 한다. 각각의 neuron 은 모두 bias와 gain을  따로 가지고 이는 non-linearity 이전에 적용이 된다. Batch normalization 과 달리 training 과 test 상의 차이가 없으며 각각의 time step 마다 따로 적용되므로 RNN에의 적용에도 적합하다.
</p>