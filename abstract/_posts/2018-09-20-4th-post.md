---
title : thu_20180920_4
layout : post
tags : [MT_Model]
---

<h3>How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures (ACL 2018)</h3>


<p><b>Tobias Domhan</b>
Amazon <br/>
Berlin, Germany<br/>
<em>domhant@amazon.com</em><br/>


</p>

<hr />
<p>
Transformer 모델은 self attention 을 통해  큰 주목을 받았지만 다른 multi head 라던지 다른 많은 attention layer 등 여러 측면이 있다. 이 논문에서는 Architecture Definition Language라는 언어를 도입하여 Transformer 를 분석하고 CNN 과 RNN 을 통해서도 Transformer 와 거의 유사한 결과를 얻을 수있음을 확인하였다. 또한 Transformer 에서는 encoder 가 decoder 보다 훨씬 중요하며 이부분은 loss 없이 CNN 혹은 RNN 으로 대체 할수있다는 것을 보여준다.
</p>