---
title : thu_20180920_5
layout : post
tags : [MT_Model]
---

<h3>The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation (ACL 2018)</h3>


<p>

<b>Melvin Johnson &nbsp;&nbsp;&nbsp;&nbsp; Wolfgang Macherey &nbsp;&nbsp;&nbsp;&nbsp; George Foster &nbsp;&nbsp;&nbsp;&nbsp; Llion Jones &nbsp;&nbsp;&nbsp;&nbsp; Niki Parmar <br/>
Noam Shazeer &nbsp;&nbsp;&nbsp;&nbsp; Ashish Vaswani &nbsp;&nbsp;&nbsp;&nbsp; Jakob Uszkoreit &nbsp;&nbsp;&nbsp;&nbsp; Lukasz Kaiser <br/>
Mike Schuster &nbsp;&nbsp;&nbsp;&nbsp; Zhifeng Chen &nbsp;&nbsp;&nbsp;&nbsp; Yonghui Wu &nbsp;&nbsp;&nbsp;&nbsp; Macduff Hughes </b> <br/>
<em>miachen,orhanf,ankurbpn,yonghui@google.com</em><br/>
Google AI <br/>



</p>

<hr />
<p>
NMT 에서 높은 성능을 보이는 최신 모델들은 모두 seq2seq 을 기반으로 한다는 공통점이 있다. 이 논문에서는 새로운 모델들과 이 모델들이 사용하는 테크닉을 두가지 방법으로 관찰했다. 첫째는 기존의 모델링과 training technique 으로 이를 RNN 에 적용하여 최신 모델을 뛰어넘는 성능을 보일수 있었다. (RNMT+) 둘째는 seq2seq의 근본적인 특성을 분석하고 이들의 강점을 결합한 새로운 모델을 고안하였다. 이 모델은 RNMT+를 뛰어넘는 성능을 보였다. 
</p>