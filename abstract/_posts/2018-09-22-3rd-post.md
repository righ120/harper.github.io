---
title : sat_20180922_3
layout : post
tags : [NN_General]
---

<h3>OUTRAGEOUSLY LARGE NEURAL NETWORKS:
THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER (ICLR 2017) </h3>


<p>

<b>Noam Shazeer<sup>1</sup>, Azalia Mirhoseini<sup>∗†1</sup>, Krzysztof Maziarz<sup>∗2</sup>, Andy Davis<sup>1</sup>, Quoc Le<sup>1</sup>, Geoffrey Hinton<sup>1</sup> and Jeff Dean<sup>1</sup></b><br/>
<sup>1</sup>Google Brain, <em>{noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com</em><br/>
<sup>2</sup>Jagiellonian University, Cracow, <em>krzysztof.maziarz@student.uj.edu.pl</em><br/>


</p>

<hr />
<p>
실험을 통해 conditinal Computation이 model의 capacity는 1000배 증가시키고, 그에 비해 computational loss는 아주 약간 밖에 일어나지 않음을 보인다. 이 논문에선 Sparsely Gated Mixture of Expers Layer를 제안하였다.(abstract 만 가지고 거의 이해가 안됨... 다음에 자세히 읽고 리뷰 예정..)
</p>