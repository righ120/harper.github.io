---
title : wed_20180926_1
layout : post
author : Heesoo Park
tags : [WE]
---

<h3>Using the Output Embedding to Improve Language Models (EACL 2017) </h3>


<p>

<b>Ofir Press and Lior Wolf</b><br/>
School of Computer Science<br/>
Tel-Aviv University, Israel<br/>
<em>{ofir.press,wolf}@cs.tau.ac.il</em><br/>

</p>

<hr />
<p>
Neural Net Language Model 에서 가장 꼭대기에 위치한 weight matrix가 word embedding vector로 사용될 수 있다는 것을 보여준다. 또한 input word embedding과 output word embedding을 동일하게 하는 것이 network의 size는 줄이고 성능은 유지할 수 있게 한다는 것을 실험적으로 증명한다.
또한 Output embedding을 정규화하는 새로운 방법을 제안한다. 이 방법으로 perplexity를 상당폭 감소 시킬 수 있었다.
</p>