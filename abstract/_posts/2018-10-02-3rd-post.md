---
title : tue_20181002_3
layout : post
author : Heesoo Park
tags : [seq2seq]
---

<h3>Universal Transformers (arXiv 2018) </h3>


<p>

<b>Mostafa Dehghani<sup>†</sup></b><br/>
University of Amsterdam<br/>
<em>dehghani@uva.nl</em><br/><br/>
<b>Stephan Gouws</b><br/>
Google Brain<br/>
<em>sgouws@google.com</em><br/><br/>
<b>Oriol Vinyals</b><br/>
DeepMind<br/>
<em>vinyals@google.com</em><br/><br/>
<b>Jakob Uszkoreit</b><br/>
Google Brain<br/>
<em>usz@google.com</em><br/>
<b>Łukasz Kaiser</b><br/>
Google Brain<br/>
<em>lukaszkaiser@google.com</em><br/>


</p>

<hr />
<p>
기존 Transformer 모델은 RNN은 쉽게 수행하는 task에서 실패하는 모습을 보였다. 또한 RNN과 달리 이론적 표현에 제한되어(?) 계산적으로 universal 하지 않다.(?) 이 논문에선 그러한 단점을 극복한다.
각각의 recurrent step에서 모든 symbol들에 표현을 반복적으로 수정하며, sequence에 다른 부분에서의 정보를 결합하기 위해 각각의 recurrent step에서 self-attention 구조를 적용한다. 
또한 adaptive computation time(ACT) 구조를 이용하여 모델이 동적으로 representation이 각각의 position에서 주성되는 횟수를 조절하도록 하였다.
<br/>

</p>