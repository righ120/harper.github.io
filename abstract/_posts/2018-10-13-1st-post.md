---
title : sat_20181013_1
layout : post
author : Heesoo Park
tags : [Language_Representaion]
---

<h3>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv 2018)</h3>


<p>

<b>Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova</b><br/>
Google AI Language<br/>
<em>{jacobdevlin,mingweichang,kentonl,kristout}@google.com</em><br/>




</p>

<hr />
<p>
source와 target의 모든 layer를 동시에 참조하여 모델을 pre-training 시킨다. 여기서 output-layer 하나만을 추가하여 fine-tuning을 하는데, 특별한 model의 변경없이 다양한 task에 맞게 fine-tuning이 가능하다. (이 부분은 multi-task learning과 연관 지을 수 있겠다.)
</p>
