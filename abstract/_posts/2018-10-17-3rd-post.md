---
title : wed_20181017_3
layout : post
author : Heesoo Park
tags : [seq2seq]
---

<h3>Accelerating Neural Transformer via an Average Attention Network (ACL 2018)</h3>


<p>

<b>Biao Zhang<sup>1,2</sup>, Deyi Xiong<sup>3</sup>and Jinsong Su<sup>1,2</sup></b><br/>
Xiamen University, Xiamen, China 361005<sup>1</sup><br/>
Beijing Advanced Innovation Center for Language Resources<sup>2</sup><br/>
Soochow University, Suzhou, China 215006<sup>3</sup><br/>
<em>zb@stu.xmu.edu.cn, dyxiong@suda.edu.cn, jssu@xmu.edu.cn</em>



</p>

<hr />
<p>
Transformer의 decoder 부분을 average attention Network를 사용하여 병렬화 시킨다. 이 Network는 2개의 layer로 이루어져 있으며, 그 중 하나는 average layer로 이전 단어에 대한 의존성을 적용하고 나머지 하나는 gate layer로 average layer 위로 stacked 되어 표현력을 강화시킨다. masking 과 dynamic programmming을 사용해 4배의 속도를 올릴 수 있었다.
</p>
