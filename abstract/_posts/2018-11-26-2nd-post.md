---
title : mon_20181126_2
layout : post
author : Heesoo Park
tags : [Optimizer_convergence]
---

<h3>ON THE CONVERGENCE OF ADAM AND BEYOND (ICLR 2018)</h3>


<p>
<b>Sashank J. Reddi, Satyen Kale & Sanjiv Kumar</b><br/>
Google New York<br/>
New York, NY 10011, USA<br/>
<em>{sashank,satyenkale,sanjivk}@google.com</em>







</p>

<hr />
<p>
gradient 기반의 optimizer가 가끔씩 converge에 실패하는 모습을 보여왔다. 이 논문에선 그 이유가 exponential moving average 때문임을 보인다. Adam이 converge에 실패하는 explicit example을 제시하고, algorithm에 long-term memory를 부여함으로써 이 문제를 해결한다.
</p>
