---
title : thu_20181129_1
layout : post
author : Heesoo Park
tags : [policy_gradient]
---

<h3>VARIANCE REDUCTION FOR POLICY GRADIENT WITH
ACTION-DEPENDENT FACTORIZED BASELINES (ICLR 2018)</h3>


<p>

<b>Cathy Wu<sup>1</sup>, Aravind Rajeswaran<sup>2</sup>, Yan Duan<sup>13</sup>, Vikash Kumar<sup>2</sup>, Alexandre M Bayen<sup>14</sup>, Sham Kakade<sup>2</sup>, Igor Mordatch<sup>3</sup>, Pieter Abbeel<sup>13</sup><br/>
<em>cathywu@eecs.berkeley.edu, aravraj@cs.washington.edu,</em><br/>
<em>rockyduan@eecs.berkeley.edu, vikash@cs.washington.edu,</em><br/>
<em>bayen@berkeley.edu, sham@cs.washington.edu,</em><br/>
<em>igor.mordatch@gmail.com, pabbeel@cs.berkeley.edu</em><br/>
1 Department of EECS, UC Berkeley<br/>
2 Department of CSE, University of Washington<br/>
3 OpenAI<br/>
4 Institute for Transportation Studies, UC Berkeley<br/>








</p>

<hr />
<p>
Policy gradient는 long horizon/high-dimension action space 내에서 high variance 문제를 겪어왔다. 이를 해결하기 위해 bias-free action-dependent baseline을 제시한다. stochastic policy의 구조적인 형태를 이용하여 variance를 줄이면서도 MDP(Markov decision process)에 대한 다른 추가 가정을 더하지 않는다.
</p>
